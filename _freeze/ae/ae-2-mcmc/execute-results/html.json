{
  "hash": "e4acf6d4691e4336f2d3493cd4a537c3",
  "result": {
    "markdown": "---\ntitle: \"MCMC\"\nauthor: \"MvO\"\ndate: '2023-11-20'\noutput:\n  html_document:\n    number_sections: yes\n    toc: yes\n  word_document: default\n  pdf_document:\n    number_sections: yes\n    toc: yes\nsubtitle: An introduction with code examples in R\n---\n\n\n# Introduction\nThis document gives a basic introduction to Markov Chain Monte Carlo sampling (MCMC). MCMC is a method for sampling from a probability distribution p(x). The method is completely generic, it can produce a representative sample from any kind of distribution. So p(x) can be continuous or discrete, univariate or multivariate.\n\n# Monte Carlo sampling\nMCMC is a specific subset of Monte Carlo sampling. We begin by demonstrating a sampling method that is MC but not MCMC.\n\nSay we want to sample from a distribution p(x) that is bivariate, and is uniform within the overlapping area of the unit square and the unit circle. We can use MC to sample from that using the following code.\n\n## EXAMPLE 1: MC-sampling using Accept-Reject (not MCMC)\n\n::: {.cell}\n\n```{.r .cell-code}\n  n         <- 5000\n  xproposed <- cbind( runif(n,0,1), runif(n,0,1) )\n  plot(xproposed, asp=1)\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n  iaccept   <- xproposed[,1]^2 + xproposed[,2]^2 < 1\n  x         <- xproposed[iaccept,]\n  plot(x, asp=1,main=\"MC-sampling\")\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n# MCMC\n\nNow we turn to MCMC.\n\nMCMC can be implemented in many different ways. Here we shall only use the oldest and simplest method: the Metropolis algorithm. The method does not require you to have a formula for your distribution (such as a probability density function). Let's use 'p(x)' to denote the distribution that you want to sample from. All you need to know to run the Metropolis algorithm is how to calculate the **ratio** of the p(x) values for any two values of x.\n\n## EXAMPLE 2: Same distribution as in EX. 1, now with MCMC\n\nWe begin with the same distribution that we already sampled using MC, but this time we sample using MCMC (Metropolis algorithm). So our distribution p(x) is again the bivariate uniform distribution within the overlap of the unit square and the unit circle.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npratio <- function(xa,xb) { (xa[1]^2 + xa[2]^2 < 1)  &&\n                            (xa[1]>=0) && (xa[2]>=0) }\n# SAMPLING FROM p(x) USING MCMC\n  n      <- 2000\n  x      <- vector(\"list\",n)\n  x[[1]] <- c(0,0)\n  for (i in 2:n) {\n     xproposed <- x[[i-1]] + runif(2,-1,1)\n     if ( runif(1,0,1) < pratio( xproposed, x[[i-1]]) )\n        x[[i]] <- xproposed\n     else\n        x[[i]] <- x[[i-1]]\n     end }\nx <- do.call(rbind,x)\n```\n:::\n\n\nLet's plot the Markov chain:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot( x[ 1: 2,],xlim=c(-0.5,1.5),ylim=c(-0.5,1.5), asp=1,\n      xlab=\"x[,1]\", ylab=\"x[,2]\") ; lines(x[1:2,],type=\"b\")\nlines(x[ 2: 3,],type=\"b\")\nlines(x[ 3: 4,],type=\"b\")\nlines(x[ 4: 5,],type=\"b\")\nlines(x[ 5: 6,],type=\"b\")\nlines(x[ 6: 7,],type=\"b\")\nlines(x[ 7: 8,],type=\"b\")\nlines(x[ 8: 9,],type=\"b\")\nlines(x[ 9:10,],type=\"b\")\nlines(x[10:11,],type=\"b\")\nlines(x[11:12,],type=\"b\")\nlines(x[12:13,],type=\"b\")\nlines(x[13:14,],type=\"b\")\nlines(x[14:15,],type=\"b\")\nlines(x[15:16,],type=\"b\")\nlines(x[16:17,],type=\"b\")\nlines(x[17:18,],type=\"b\")\nlines(x[18:19,],type=\"b\")\nlines(x[19:20,],type=\"b\")\nfor(i in 21:n) { lines(x[(i-1):i,],type=\"b\") }\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe see that the MCMC gives the same result as the MC.\n\n## EXAMPLE 3 Univariate distribution\n\nWe now turn to a distribution where the ratio is more complicated.\n\n- p(x): univariate, unknown distribution on [-Inf,Inf]\n- p(xa)/p(xb) = exp(xb^2-xa^2)\n- MCMC-sampling from p(x) using Metropolis\n\n\n::: {.cell}\n\n```{.r .cell-code}\npratio <- function(xa,xb) { exp(xb^2-xa^2) }\n# SAMPLING FROM p(x) USING MCMC\n  n      <- 5000\n  x      <- vector(\"list\",n)\n  x[[1]] <- 0\n  for (i in 2:n) {\n     xproposed <- x[[i-1]] + runif(1,-1,1)\n     if ( runif(1,0,1) < pratio( xproposed, x[[i-1]]) )\n        x[[i]] <- xproposed\n     else\n        x[[i]] <- x[[i-1]]\n     end }\nx <- do.call(rbind,x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\n\nplot(x,type=\"b\")\nhist(x) ; print(mean(x)) ; print(sd(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.004955363\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6992098\n```\n:::\n\n```{.r .cell-code}\n# In this case, R could have given the answer directly\nxdirect <- rnorm(n,0,0.7)\nplot(xdirect)\nhist(xdirect) ; print(mean(xdirect)) ; print(sd(xdirect))\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.008299763\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6923803\n```\n:::\n:::\n\n\n## EXAMPLE 4: Bivariate distribution\n\n- p(x): bivariate, unknown distribution on the real number plane\n- p(xa)/p(xb) = exp(xb[1]^2-xa[1]^2) * exp(xb[2]^2-xa[2]^2)\n- MCMC-sampling from p(x) using Metropolis\n\n\n::: {.cell}\n\n```{.r .cell-code}\npratio <- function(xa,xb) { exp(xb[1]^2-xa[1]^2) * exp(xb[2]^2-xa[2]^2) }\n# SAMPLING FROM p(x) USING MCMC\n  n      <- 2000\n  x      <- vector(\"list\",n)\n  x[[1]] <- c(0,0)\n  for (i in 2:n) {\n     xproposed <- x[[i-1]] + runif(2,-1,1)\n     if ( runif(1,0,1) < pratio( xproposed, x[[i-1]]) )\n        x[[i]] <- xproposed\n     else\n        x[[i]] <- x[[i-1]]\n     end }\nx <- do.call(rbind,x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar( mfrow=c(1,2) )\n\nplot(x,asp=1,type=\"b\")\n# In this case too, R could have given the answer directly\nrequire(mvtnorm) ; xdirect <- rmvnorm( n, c(0,0), diag(c(0.5,0.5)) )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mvtnorm\n```\n:::\n\n```{.r .cell-code}\nplot(xdirect,asp=1)\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## EXAMPLE 5: Disjoint distribution\n\nWe now show that MCMC can sample from multimodal distributions, even when the modal regions are separated by a region of zero-probability.\n\n- p(x): bivariate, disjoint, unknown distribution on the real number plane\n- p(xa)/p(xb) = (xa[1]^2 + xa[2]^2 < 1) || ((xa[1]-2)^2 + (xa[2]-0.5)^2 < 0.5)\n- MCMC-sampling from p(x) using Metropolis\n\n\n::: {.cell}\n\n```{.r .cell-code}\npratio <- function(xa,xb) {  (xa[1]   ^2 +  xa[2]     ^2 < 1  ) ||\n                            ((xa[1]-2)^2 + (xa[2]-0.5)^2 < 0.5)    }\n# SAMPLING FROM p(x) USING MCMC\n  n      <- 2000\n  x      <- vector(\"list\",n)\n  x[[1]] <- c(0,0)\n  for (i in 2:n) {\n     xproposed <- x[[i-1]] + runif(2,-1,1)\n     if ( runif(1,0,1) < pratio( xproposed, x[[i-1]]) )\n        x[[i]] <- xproposed\n     else\n        x[[i]] <- x[[i-1]]\n     end }\nx <- do.call(rbind,x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar( mfrow=c(1,1) )\nplot(x[1:2,],xlim=c(-1,3),ylim=c(-1,1.5),asp=1) ; lines(x[1:2,],type=\"b\")\nfor(i in 3:n) { lines(x[(i-1):i,],type=\"b\",lty=3) }\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(x,xlim=c(-1,3),ylim=c(-1,1.5),asp=1)\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n# Bayesian regression\n\n## Why is MCMC so useful for Bayesian modelling?\n\nBayes' Theorem tells us how to use data D to change a prior into a posterior: p(x|D) = p(x) p(D|x) / p(D). There is usually no problem specifying a prior p(x) and a likelihood function L(x) = p(D|x). But we usually do not know what p(D) is. So we cannot write down the formula for the posterior. But we can still use MCMC because the troublesome p(D) terms cancel out in the ratio p(xa|D) / p(xb|D) for any pair of points xa and xb in parameter space. The ratio of posterior probabilities for xa and xb is thus the ratio of \"prior times likelihood\" for xa and xb:\n\np(xa|D) / p(xb|D) = p(xa) p(D|xa) / (p(xb) p(D|xb)).\n\nBecause that posterior ratio is usually known for any pair of points, MCMC can be used to sample from posterior distributions.\n\n## Bayesian linear regression\n\nWe begin by getting our data, defining our likelihood function, model and prior, and the settings of the MCMC that we are going to do.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(mvtnorm)\nchainLength <- 20000\ndata <- matrix( c(10,  6.09, 1.83,\n                  20,  8.81, 2.64,\n                  30, 10.66, 3.27),\n                nrow=3, ncol=3, byrow=T) ; nd <- nrow(data)\nLi   <- function( model, theta, data, i ) {\n          dnorm( (model(data[i,1],theta)-data[i,2])/data[i,3] ) }\nL    <- function( model, theta, data    ) {\n          prod( sapply(1:nd,function(i){Li(model,theta,data,i)}) ) }\nlogL <- function( model, theta, data  ) { log(L(model,theta,data)) }\n# We define a function 'model' that is the straight line:\nmodel       <- function (x,theta) { theta[1]*x + theta[2] }\nnp           <- 2\nprior        <- matrix( c(0, 1,\n                          0,10), nrow=np, byrow=T )\npMin         <- prior[,1] ; pMax <- prior[,2]\npVector      <- rowMeans(prior)\npChain       <- matrix(, nrow=chainLength, ncol=np)\npChain[1,]   <- pVector\nlogPrior0    <- sum( log( dunif(pVector,pMin,pMax) ) )\n\nlogL0        <- logL(model,pVector,data)\nlogLChain    <- matrix(, nrow=chainLength, ncol=1)\nlogLChain[1] <- logL0\n\nvcovProposal <- diag( ( 0.2*(pMax-pMin) )^2 )\n```\n:::\n\n\nNow we are ready to run the MCMC itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (c in 2:chainLength) {\n  cint <- min(1000,round(chainLength/10))\n  if (c%%cint == 0) cat(\"Iteration\",c,\"|\",logPrior0,\"|\",logL0,\"\\n\")\n  candidatepVector <- rmvnorm(n=1, mean=pVector, sigma=vcovProposal)\n  logPrior1   <- sum(log(dunif(candidatepVector, pMin, pMax)))\n  if (!is.na(logPrior1)) { \n    logL1    <- logL(model,candidatepVector,data)\n    logalpha <- logPrior1 + logL1 - logPrior0 - logL0\n    if (log(runif(1, min=0, max=1)) < logalpha) {\n      pVector   <- candidatepVector\n      logPrior0 <- logPrior1\n      logL0     <- logL1                       } }\n     pChain[c,]    <- pVector\n     logLChain[c,] <- logL0   }\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 1000 | -2.302585 | -3.495158 \nIteration 2000 | -2.302585 | -3.500235 \nIteration 3000 | -2.302585 | -2.87472 \nIteration 4000 | -2.302585 | -3.672384 \nIteration 5000 | -2.302585 | -4.422576 \nIteration 6000 | -2.302585 | -3.23637 \nIteration 7000 | -2.302585 | -3.016112 \nIteration 8000 | -2.302585 | -3.083251 \nIteration 9000 | -2.302585 | -3.24425 \nIteration 10000 | -2.302585 | -4.255816 \nIteration 11000 | -2.302585 | -3.642816 \nIteration 12000 | -2.302585 | -3.07823 \nIteration 13000 | -2.302585 | -4.081337 \nIteration 14000 | -2.302585 | -2.786922 \nIteration 15000 | -2.302585 | -2.918381 \nIteration 16000 | -2.302585 | -3.626493 \nIteration 17000 | -2.302585 | -3.771179 \nIteration 18000 | -2.302585 | -3.291751 \nIteration 19000 | -2.302585 | -4.551895 \nIteration 20000 | -2.302585 | -3.349065 \n```\n:::\n:::\n\n\nLet's examine the MCMC-results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnAccepted  <- length(unique(logLChain))\nacceptance <- (paste(nAccepted, \"out of \", chainLength,\n                     \"candidates accepted ( = \",\n                     round(100*nAccepted/chainLength), \"%)\"))\nprint(acceptance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"6233 out of  20000 candidates accepted ( =  31 %)\"\n```\n:::\n\n```{.r .cell-code}\nmp        <- colMeans(pChain)                ; print(mp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2408432 3.8519836\n```\n:::\n\n```{.r .cell-code}\niMAP      <- match(max(logLChain),logLChain)\nMAP       <- pChain[iMAP,]                  ; print(MAP)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2354268 3.7975535\n```\n:::\n\n```{.r .cell-code}\npCovMatrix<- cov(pChain)                     ; print(pCovMatrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]      [,2]\n[1,]  0.01556034 -0.214535\n[2,] -0.21453499  4.531865\n```\n:::\n\n```{.r .cell-code}\npCorMatrix<- cor(pChain)                     ; print(pCorMatrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]       [,2]\n[1,]  1.0000000 -0.8078862\n[2,] -0.8078862  1.0000000\n```\n:::\n\n```{.r .cell-code}\nsp        <- sqrt( diag(pCovMatrix) )       ; print(sp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1247411 2.1288177\n```\n:::\n\n```{.r .cell-code}\nplot  ( 1:30,model(1:30,mp ), col=\"black\",lwd=3,type=\"l\" )\nlines ( 1:30,model(1:30,MAP), col=\"red\"  ,lwd=3,type=\"p\" )\npoints( data                  , col='blue' ,lwd=3,cex=2    )\n```\n\n::: {.cell-output-display}\n![](ae-2-mcmc_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n# EXERCISES\n\nAll these exercises ask you to modify the MCMC-code for linear regression. We want to keep the original code, so copy the code-chunks to this section (or to a new Rmd-file), and modify that copy.\n\n1. Make the data more informative by reducing the standard deviation by a factor 10. What effects does that have on the Bayesian calibration of the straight-line model?\n2. Same question but **increasing** the uncertainty by a factor 10.\n3. If you would make the uncertainty about one of the three data points much higher than for the other points, how would that affect the calibration?\n4. Modify the code for linear regression: replace the straight line (2 parameters) by a quadratic curve (3 parameters). Can you get the code to work? How would you decide whether the linear or the quadratic curve is the best one? How would you define 'best' in a Bayesian way?\n",
    "supporting": [
      "ae-2-mcmc_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}